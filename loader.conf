# FreeBSD 11 -- /boot/loader.conf
# date: 20170406
#
# low latency is important so we highly recommend that you disable hyper
# threading on Intel CPUs as it has an unpredictable affect on latency, cpu
# cache misses and load. 

#Part1: Kernel

# How many seconds to sit at the boot menu before booting the server. Reduce
# this value for a faster booting machine. For a server, you may want to
# increase this time if you have the BIOS auto boot after a power outage or
# brownout. By increasing the delay you allow more time for the power grid to
# stabilize and UPS batteries to re-charge. Ideally, you want to avoid the
# system fast booting into the OS and mounting the file system only to power
# off because of another brownout. If you are at the console during boot you
# can always hit enter to bypass this delay. (default 10 seconds)
autoboot_delay="5"

# when booting, display the ascii art FreeBSD Orb with the two horns on top.
# Just a cosmetic preference over "beastie", the multicolored daemon with
# pitchfork and oversized shoes.
#loader_logo="orb"

# higher HZ settings have a negative impact on machine performance due to
# handling more timer interrupts resulting in more context switches and cache
# flushes (default 1000).  Lower HZ settings can have a detrimental effect on
# ZFS.
# http://lists.freebsd.org/pipermail/freebsd-questions/2005-April/083482.html
# Also take a look into kern.sched.interact and kern.sched.slice in
# /etc/sysctl.conf
kern.hz="1000"

# Change the zfs pool output to show the GPT id for each drive instead of the
# gptid or disk identifier. The gpt id will look like "ada0p2"; the gpt id
# "ada0" of the first drive found on the AHCI SATA / SAS / SCSI chain and
# partition string "p2".  Use "gpart list" to see all drive identifiers and
# "zpool status" to see the chosen id through ZFS.
kern.geom.label.disk_ident.enable="1"
kern.geom.label.gpt.enable="1"
kern.geom.label.gptid.enable="1"

# If your server has lots of swap (>4Gb) you should increase following value
# according to http://lists.freebsd.org/pipermail/freebsd-hackers/2009-October/029616.html
# Otherwise you'll be getting errors
# "kernel: swap zone exhausted, increase kern.maxswzone"
kern.maxswzone="256M" 

# Useful for databases 
# Sets maximum data size to 1G
# (FreeBSD 7.2+ amd64 users: Check that current value is lower!)
#kern.maxdsiz="1G"

# Maximum buffer size(vfs.maxbufspace)
# You can check current one via vfs.bufspace
# Should be lowered/upped depending on server's load-type
# Usually decreased to preserve kmem
# (default is 10% of mem)
#kern.maxbcache="512M"
kern.maxproc="32768"
kern.ipc.maxsockets="262144"
kern.ipc.nmbclusters="492680"
kern.maxfiles="10485760"

# Shared memory
# Note: Only FreeBSD 7.2+ can use shared memory > 2Gb
kern.ipc.shmmni="4096"
kern.ipc.shmall="3294967296"
kern.ipc.shmmax="1294967296"

#Part2: Memory

# Increase kernel memory size to 3G.
#
# Use ONLY if you have KVA_PAGES in kernel configuration, and you have more than 3G RAM
# Otherwise panic will happen on next reboot!
#
# It's required for high buffer sizes: kern.ipc.nmbjumbop, kern.ipc.nmbclusters, etc
# Useful on highload stateful firewalls, proxies or ZFS fileservers
# (FreeBSD 7.2+ amd64 users: Check that current value is lower!)
#vm.kmem_size="3G"
# Enable superpages, for 7.2+ only
# Also read http://lists.freebsd.org/pipermail/freebsd-hackers/2009-November/030094.html
vm.pmap.pg_ps_enabled="1"

#Part3: Harware

# Adds NCQ support in FreeBSD
# WARNING! all ad[0-9]+ devices will be renamed to ada[0-9]+
# 8.0+ only
#ahci_load="YES"
#siis_load="YES"
mfip_load="YES"

# thermal sensors for intel or amd cpus
#coretemp_load="YES"
#amdtemp_load="YES"

# enable /dev/crypto for IPSEC of custom seeding using the AES-NI Intel
# hardware cpu support
#aesni_load="YES"

# maximum number of interrupts per second on any interrupt level (vmstat -i for
# total rate). If you still see Interrupt Storm detected messages, increase the
# limit to a higher number and look for the culprit.  For 10gig NIC's set to
# 9000 and use large MTU. (default 1000)
hw.intr_storm_threshold="9000"

# load the Intel PRO/1000 PCI Express kernel module on boot
#if_igb_load="YES"

# load the Myri10GE kernel module on boot
#if_mxge_load="YES"

# load the PF CARP module
#if_carp_load="YES"

######################################### intel igb tuning ##############

# Once of the best upgrades for a network server is to replace the network
# interface with an efficient network card. The onboard chipsets use a
# significant amount of CPU time. By simply installing an Intel i350 network
# card you can reduce CPU time and interrupt processing by 400% and reduce
# latency.

# the following hw.igb.* are for the Intel i350 nic ( igb0, igb1, ect. ) 
# https://www.myricom.com/software/myri10ge/432-how-do-i-achieve-the-lowest-possible-latency-with-myri10ge.html
# http://bsdrp.net/documentation/examples/forwarding_performance_lab_of_an_ibm_system_x3550_m3_with_intel_82580
# http://fasterdata.es.net/network-tuning/router-switch-buffer-size-issues/

# maximum number of interrupts per second generated by single igb(4) (default
# 8000). FreeBSD 10 supports the new drivers which reduces interrupts
# significantly.
#hw.igb.max_interrupt_rate="32000" # (default 8000)

# number of queues supported on the hardware NIC (default 0); the Intel i350
# supports up to eight(8) queues. For saturated networks, set to zero(0) to
# allow the driver to auto tune and create as many queues as real CPU cores up
# to the NIC's maximum. Make sure to always disable hyperthreading. FreeBSD 10
# has the new igb(0) driver which significantly reduces the amount of
# inturrepts on the NIC. For lightly loaded networks on FreeBSD 9 and earlier,
# test setting to one(1) to reduce interrupts, lower latency and increase
# efficiency. Then check interrupts per second with "vmstat -i" and try to get
# the counts as low as possible. FreeBSD is most efficient using multiple
# queues and network threading.
#hw.igb.num_queues="0" # (default 0)

# Intel igb(4): The maximum number of packets to process at Recieve End Of
# Frame (RxEOF). A frame is a data packet on Layer 2 of the OSI mode and "the
# unit of transmission in a link layer protocol consisting of a link-layer
# header followed by a packet." "-1" means unlimited. The default of "100" can
# processes around 500K pps, so only set to -1 is you are accepting 500K
# packets per second or more. Test with "netstat -ihw 1" and look at packets
# recieved per second.
# http://bsdrp.net/documentation/examples/forwarding_performance_lab_of_an_ibm_system_x3550_m3_with_intel_82580
#hw.igb.rx_process_limit="-1"  # (default 100)

# Intel igb(4): Adaptive interrupt Moderation adjusts the interrupt rate
# dynamically based on packet size and throughput and reduces system load for
# igb(4). Enabling AIM, and the seperate MSIX option, will result in
# significantly better effciency in the network stack.
#hw.igb.enable_aim="1"  # (default 1)

# Intel igb(4): MSI-X interrupts for PCI-E devices permiting a device to
# allocate up to 2048 device interrupts per CPU interrupt. A separate MSI-X
# vector is allocated for each queue and one for "other" interrupts such as
# link status change and errors. Enabling an MSI-X device will improve network
# efficiency by at least 400%. MSI-X allows a larger number of interrupts and
# gives each one a separate target address and data word. Verify MSI-X is being
# used by the NIC using "dmesg | grep -i msi" with the output looking similar
# to, "igb0: Using MSIX interrupts with 5 vectors" for a two(2) port, four(4)
# queue Intel i350-T2 network card.
#hw.igb.enable_msix="1"  # (default 1)

# Intel igb(4): Intel PRO 1000 network chipsets support a maximum of 4096 Rx
# and 4096 Tx descriptors. Two cases when you could change the amount of
# descriptors are: 1) Low RAM and 2) CPU or bus saturation. If the system RAM
# is too low you can drop the amount of descriptors to 128, but the system may
# drop packets if it can not proceeses the packets fast enough. If you have a
# large number of packets incoming and they are being processed too slowly then
# you can increase to the descriptors up to 4096. Increasing descriptors is
# only a hack because the system is too slow to processes the packets in a
# timely manner. You should look into getting a faster CPU with a wider bus or
# identifying why the recieving application is so slow. Use "netstat -ihw 1"
# and look for idrops. Note that each received packet requires one Receive
# Descriptor, and each descriptor uses 2 KB of memory.
# https://fasterdata.es.net/host-tuning/nic-tuning/
#hw.igb.rxd="2048"  # (default 1024)
#hw.igb.txd="2048"  # (default 1024)

# Intel igb(4): using older intel drivers and jumbo frames caused memory
# fragmentation as header splitting wouldn't allocate jumbo clusters. The
# current intel drivers do not seem to have these issues, so headers splitting
# is disabled by default.
#hw.igb.header_split=0 # (default 0)

# Intel igb(4): The Intel i350 dual port NIC supports up to eight(8)
# input/output queues per port. On average, a single CPU core can forward 700K
# packets per second (pps) and a gigabit interface can forward 1.488M packets
# per second (pps). On a four(4) core firewall/router with two(2) igb gigabit
# links we found setting the NIC to two(2) queues per port is most efficient.
# We recommend saving at least one(1) CPU core for userland processing. Query
# total interrupts per queue with "vmstat -i" and use "top -H -S" to watch CPU
# usage per igb0:que. MSIX interrupts start at 256 and the igb driver uses one
# vector per queue known as an TX/RX pair. The default value of zero(0) sets
# the number of queues equal to the number of logical CPU cores. Disable
# hyperthreading as HT logical cores are not used in routing and simultaneous
# multithreading (SMT) can lead to unpredictable latency spikes.
#hw.igb.num_queues="2" # (default 0)

#Part4: Filesystem

# Asynchronous I/O, or non-blocking I/O is a form of input/output processing
# permiting other processing to continue before the transmission has finished.
# AIO is used for accelerating Nginx on ZFS. Check for our tutorials on both.
aio_load="YES"

# ZFS root boot config
zfs_load="YES"
#vfs.root.mountfrom="zfs:bsdzfs/root"

############################################ zfs tuning begin ##############
# martin_matuska_eurobsdcon_2012 = http://www.youtube.com/watch?v=PIpI7Ub6yjo
#
# The goal is to keep as much data in RAM before committing to maximize long,
# concurrent writes and reduce data fragmentation. This machine has eight(8)
# gigabytes of RAM with zfs mirrored 4 TB drives.

# Dynamically adjust max write limit based on previous txg commits to attempt
# to maintain a 3-second commit time. If the SATA based mirrored pool can write
# at 120 MB/sec then the goal is to keep at least (120 MB/sec times 3 seconds
# equals) 360 MB of data in the write cache to be written to the pool all at
# once.
vfs.zfs.txg.synctime_ms="15000"  # default 1000

# Commit async writes if the maximum I/O requests pending on each device reach
# the limit.
vfs.zfs.vdev.max_pending="4096"  # default 10

# Commit async writes after 120 seconds if the max write limit is not reached.
# WARNING: in the worst case scenario we could loose all 120 seconds worth of
# data if the machine is abruptly powered off or looses power.
vfs.zfs.txg.timeout="900"  # default 5 seconds

# Increase VDEV read ahead cache size. This reduces scrub times and
# metadata-intensive tasks for a small cost in RAM. The vdev cache uses a
# simple FIFO rolling data set.
vfs.zfs.vdev.cache.size="256M"
vfs.zfs.vdev.cache.max="65536" 

# Default vfs.zfs.write_limit_shift appears to be "3" which on a system
# with 2GB RAM such as this one results in a write_limit_max of 256MB, which
# is appropriate, so we're not going to change that here.
#vfs.zfs.write_limit_shift="3"
# disables metadata sync mode and uses async I/O without flushes
vfs.zfs.cache_flush_disable="1"

#Part5: Networking 

# Pf firewall kernel modules, preload
#pf_load="YES"
#pflog_load="YES"

# accf accept filters are used so the server will not have to context switch several times
# before performing the initial parsing of the request. This could decrease server load by
# reducing the amount of CPU time to handle incoming requests.

# Wait for data accept filter. For nginx https servers add "listen
# 127.0.0.1:443 ssl spdy accept_filter=dataready;"
accf_data_load="YES"

# buffer incoming connections until complete HTTP requests arrive (nginx
# apache) for nginx http add, "listen 127.0.0.1:80 accept_filter=httpready;"
accf_http_load="YES"

# Wait for full DNS request accept filter (unbound)
accf_dns_load="YES"

# New Congestion Control for FreeBSD
# http://caia.swin.edu.au/urp/newtcp/tools/cc_chd-readme-0.1.txt
# http://www.ietf.org/proceedings/78/slides/iccrg-5.pdf
# Initial merge commit message http://www.mail-archive.com/svn-src-all@freebsd.org/msg31410.html
#cc_chd_load="YES"

# CUBIC Congestion Control allows for more fairness between flows since the
# window growth is independent of RTT.
#cc_cubic_load="YES"

# H-TCP Congestion Control for a more aggressive increase in speed on higher
# latency, high bandwidth networks with some packet loss. 
cc_htcp_load="YES"

# hostcache cachelimit is the number of ip addresses in the hostcache list.
# Setting the value to zero(0) stops any ip address connection information from
# being cached and negates the need for "net.inet.tcp.hostcache.expire". We
# find disabling the hostcache increases burst data rates by 2x if a subnet was
# incorrectly graded as slow on a previous connection. A host cache entry is
# the client's cached tcp connection details and metrics (TTL, SSTRESH and
# VARTTL) the server can use to improve future performance of connections
# between the same two hosts. When a tcp connection is completed, our server
# will cache information about the connection until an expire timeout. If a new
# connection between the same client is initiated before the cache has expired,
# the connection will use the cached connection details to setup the
# connection's internal variables. This pre-cached setup allows the client and
# server to reach optimal performance significantly faster because the server
# will not need to go through the usual steps of re-learning the optimal
# parameters for the connection. To view the current host cache stats use
# "sysctl net.inet.tcp.hostcache.list" 
net.inet.tcp.hostcache.cachelimit="1966080"

# Interface Maximum Queue Length: common recomedations are to set the interface
# buffer size to the number of packets the interface can transmit (send) in 50
# milliseconds _OR_ 256 packets times the number of interfaces in the machine;
# whichever value is greater. To calculate a size of a 50 millisecond buffer
# for a 60 megabit network take the bandwidth in megabits divided by 8 bits
# divided by the MTU times 50 millisecond times 1000, 60/8/1460*50*1000=256.84
# packets in 50 milliseconds. OR, if the box has two(2) interfaces take 256
# packets times two(2) NICs to equal 512 packets. 512 is greater then 256.84 so
# set to 512.
#
# Our preference is to define the interface queue length as two(2) times the
# value set in the interface transmit descriptor ring, "hw.igb.txd". If
# hw.igb.txd="1024" then set the net.link.ifqmaxlen="2048". 
#
# An indirect result of increasing the interface queue is the buffer acts like
# a large TCP initial congestion window (init_cwnd) by allowing a network stack
# to burst packets at the start of a connection. Do not to set to zero(0) or
# the network will stop working due to "no network buffers" available. Do not
# set the interface buffer ludicrously large to avoid buffer bloat.
net.link.ifqmaxlen="2048"  # (default 50)

# Size of the syncache hash table, must be a power of 2 (default 512)
net.inet.tcp.syncache.hashsize="32768"

# Limit the number of entries permitted in each bucket of the hash table. (default 30)
net.inet.tcp.syncache.bucketlimit="100"
net.inet.tcp.syncache.cachelimit="1048576"

# number of hash table buckets to handle incoming tcp connections. a value of
# 65536 allows the system to handle millions incoming connections. each tcp
# entry in the hash table on x86_64 uses 252 bytes of ram.  vmstat -z | egrep
# "ITEM|tcpcb" (default 65536 which is ~16M connections)
net.inet.tcp.tcbhashsize="524288"

######################################### net.isr. tuning begin ##############
# NOTE regarding "net.isr.*" : Processor affinity can effectively reduce cache
# problems but it does not curb the persistent load-balancing problem.[1]
# Processor affinity becomes more complicated in systems with non-uniform
# architectures. A system with two dual-core hyper-threaded CPUs presents a
# challenge to a scheduling algorithm. There is complete affinity between two
# virtual CPUs implemented on the same core via hyper-threading, partial
# affinity between two cores on the same physical chip (as the cores share
# some, but not all, cache), and no affinity between separate physical chips.
# It is possible that net.isr.bindthreads="0" and net.isr.maxthreads="3" can
# cause more slowdown if your system is not cpu loaded already. We highly
# recommend getting a more efficient network card instead of setting the
# "net.isr.*" options. Look at the Intel i350 for gigabit or the Myricom
# 10G-PCIE2-8C2-2S for 10gig. These cards will reduce the machines nic
# processing to 12% or lower.

# Max number of threads for NIC IRQ balancing. Set to the number of real cpu
# cores in the box. maxthreads will spread the load over multiple cpu cores at
# the the cost of cpu affinity unbinding. The default of "1" should be faster if
# the machine is mostly idle.
net.isr.maxthreads="8" # (default 1)
net.isr.numthreads="4"

# qlimit for igmp, arp, ether and ip6 queues only (netstat -Q) (default 256)
net.isr.defaultqlimit="2048" # (default 256)

# For high bandwidth systems settting bindthreads to "0" will spread the
# network processing load over multiple cpus allowing the system to handle more
# throughput. The default is faster for most lightly loaded systems (default 0)
net.isr.bindthreads="0"

# interrupt handling via multiple CPU (default direct)
net.isr.dispatch="direct"

# limit per-workstream queues (use "netstat -Q" if Qdrop is greater then 0
# increase this directive) (default 10240)
net.isr.maxqlimit="10240"

# SIFTR (Statistical Information For TCP Research) is a kernel module which
# logs a range of statistics on active TCP connections to a log file in comma
# separated format. Only useful for researching tcp flows as it does add some
# processing load to the system.
# http://manpages.ubuntu.com/manpages/precise/man4/siftr.4freebsd.html
#siftr_load="YES"


